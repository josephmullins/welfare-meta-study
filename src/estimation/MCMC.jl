# the basis of estimation is the quasi-likelihood:

# G(Œ∏_0) ‚àº N(0,V[G])
# and so: G(Œ∏_0)V^{-1}G() is...?
# 
using Turing, LinearAlgebra

# would have to change this if adding type-specific parameters
function get_zxb!(zxb,pars,data)
    thB,thC = predict_skills(pars,data)
    for iz in eachindex(data.Z)
        zxb[pos] += thB * data.Z[iz]
        pos += 1
    end
    for iz in eachindex(data.Z)
        zxb[pos] += thC * data.Z[iz]
        pos += 1
    end
end


@model function quasilikelihood(zy,data,factor_scores)
    num_X = size(data[1].X,1)
    Œ¥I ~ [Uniform(0,3) for i in 1:2]
    Œ¥Œ∏ ~ [Uniform(0.5,1) for i in 1:2]
    g ~ [Uniform(-2,2) for i in 1:2, j in 1:2]
    Œ≤ ~ [Flat() for i in 1:num_X]
    pars = (Œ¥I = Œ¥I,Œ¥Œ∏ = Œ¥Œ∏,g = g, Œ≤ = Œ≤)
    # get variance
    V = moment_variance(pars,data,factor_scores) / N
    zxb = zeros(eltype(Œ¥I))
    for d in data
        get_zxb!(zxb,pars,d)
    end
    N = length(data)
    zy ~ MvNormal(zxb / N,V)
end

@model function test_like(y,x)
    Œ≤ ~ MultivariateNormal(zeros(3),I(3))
    for i in eachindex(y)
        y[i] ~ Normal(dot(x[:,i],Œ≤),1)
    end
end

@model function test_like(data)
    Œ≤ ~ MultivariateNormal(zeros(3),I(3))
    for d in data
        d.y ~ Normal(dot(d.X,Œ≤),1)
    end
end

N = 1000
Œ≤0 = [1.,0.,0.5]
x = rand(3,N)
Fœµ = Normal()
y = x'*Œ≤0 .+ rand(Fœµ,N)
data = [(X = x[:,i],y = dot(Œ≤0,x[:,i])+rand(Fœµ)) for i in 1:N]

sample(test_like(data),NUTS(),1000)

@model function test_like(y,data)
    Œ≤ ~ MultivariateNormal(zeros(3),I(3))
    #r = zeros(eltype(Œ≤),length(y))
    N = length(y)
    r = zero(eltype(Œ≤))
    for i in eachindex(data)
        r += y[i] - dot(data[i].X,Œ≤)
        #y[i] ~ Normal(dot(data[i].X,Œ≤),1)
    end
    r /= N
    r ~ Normal(0,1/N)
end

sample(test_like(y,data),NUTS(),1000)

# ---- test:
N = 1000
Œ≤0 = [1.,0.,0.5]
z = rand(N,3)
Œ≥ = rand(3,3)
x = z * 3 + 0.2*rand(Normal(),N,3)
Fœµ = Normal()
y = x*Œ≤0 .+ rand(Fœµ,N)

zy = (z' * y)[:] / N
zx = z' * x / N
V = z' * z / N^2

# let's test a quasilikelihood:
# Var[1 / N ‚àë[(y-xŒ≤)x]] = (1 / N) œÉ¬≤ ùîº[x'x]
#!?
# hmm. not sure about this? can we do thinks this way?
# CLT says: g(X,Œ≤) ‚Üí_d N(0,V[g])

@model function moment_like(zy,zx,V)
    Œ≤ ~ MultivariateNormal(zeros(3),I(3))
    zy ~ MultivariateNormal(zx*Œ≤,V)
end

sample(moment_like(zy,zx,V),NUTS(),1000)

# NEXT: do the same as above but with V "estimated" and updated each time.
# this is it. it works. job now is to figure out the domain stuff.
# NOTE: this also shows that Turing already does the bijection for you
@model function moment_like(zy,zx,y,z,x,::Type{T} = Float64) where {T}
    #Œ≤ ~ MultivariateNormal(zeros(3),I(3))
    #Œ≤ ~ fill(Uniform(-2,2),3)
    Œ≤ = Vector{T}(undef,3)
    for i in 1:3
        Œ≤[i] ~ Uniform(-2,2)
    end
    e = y - x * Œ≤
    V = (z'z / N) * (e'e / N)
    zy ~ MultivariateNormal(zx*Œ≤,V / N)
end

chain = sample(moment_like(zy,zx,y,z,x),NUTS(),1000)
